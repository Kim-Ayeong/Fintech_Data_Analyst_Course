{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Kim-Ayeong/Fintech_Data_Analyst_Course/blob/master/Textbook5/notebook/1.%20n-Gram_200128.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from numpy.random import randint, seed\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from PIL import Image  \n",
    "from IPython.display import set_matplotlib_formats\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Python\\\\4-2) 핀테크 교육\\\\Final Project\\\\Project1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Python\\\\4-2) 핀테크 교육\\\\Final Project\\\\Project1\\\\data\\\\total'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\".\\data\\total\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13738, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text=pd.read_excel(\"교양오락문화생활비.xlsx\", encoding='CP949')\n",
    "my_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['김준호 유튜브 채널 구독자 38만명 돌파…뼈그맨 인증',\n",
       "       '\\'두데\\' 감스트 \"구독자들 \\'야\\'라고 불러…난 \\'인직아\\'라고 불린다\"',\n",
       "       '[뉴스레터 구독 이벤트] 1차 당첨자 추첨 과정 공개…100명의 행운의 주인공은...',\n",
       "       '[MT리포트] \"1년 수입 10억\"…수백만 구독자 \\'유튜버의 세계\\'',\n",
       "       'CJ E&M, ‘다이아 티비’ 구독자 1억 6000만명 돌파',\n",
       "       \"'슈가맨' '히든싱어' 등 음악예능 인기에…JTBC 유튜브 구독자 200만 돌파\",\n",
       "       \"250만 구독자 '유튜버 망치'한국의 맛 알려\",\n",
       "       \"[뉴스레터 구독 이벤트] RC카 '러커스'로 대리만족 한 번 해보시렵니까\",\n",
       "       \"[뉴스레터 구독 이벤트] 1차를 놓쳤다면 'V30'가 기다리는 2차 이벤트에 도전\",\n",
       "       '250만 구독자 유튜버가 전하는 한국의 맛'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text['title'].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_text['title'].values) - len(set(my_text['title'].values)) # 중복 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_meaning = \"구독|책|핫플레이스|핫|플레이스|레이스|휴가비\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['title'].values):\n",
    "    a_line = re.sub('[^ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)           # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)               # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['천년의 숲 비자림과 국내 최고 당근 마을',\n",
       " ' 생활건강 온더바디 새 모델에 배우 유인나 발탁',\n",
       " ' 의해 기념 북콘서트 나는 기억이다 ',\n",
       " ' 년 섭외 순위 박나래의 활약이 기대되는 이유',\n",
       " ' 읽는 세종 올해도 만 양서 확충',\n",
       " '어린이날 행사 풍성 평화가족한마당',\n",
       " '한해 양다일 연애포차 사실은 일 공개',\n",
       " ' 골목식당 백종원 국수집 이을 대환장 식당 등장 폐업까지 고려 ',\n",
       " '전북대 학생들 전북 전승설화 조사해 쓰다',\n",
       " ' 힘들때 손 내미는 게 진정한 후원 박인비처럼 골프도 스타 키울 것 ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "# 명사 추출\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "    \n",
    "#my_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단음절 제거\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word\n",
    "        \n",
    "#my_words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정규직 전환', 103),\n",
       " ('국정원 특활비', 73),\n",
       " ('근로자 휴가지원사업', 60),\n",
       " ('만명 돌파', 56),\n",
       " ('임금협약 체결', 50),\n",
       " ('박서준 박민영', 42),\n",
       " ('김비서 박서준', 35),\n",
       " ('박근혜 국정원', 34),\n",
       " ('노사 임단협', 33),\n",
       " ('만원 지원', 33),\n",
       " ('정규직 정규직', 33),\n",
       " ('한국 노사', 33),\n",
       " ('서울시장 후보', 32),\n",
       " ('국정원 뇌물', 28),\n",
       " ('이민우 여진구', 28),\n",
       " ('문고리 인방', 26),\n",
       " ('캘린더 서비스', 25),\n",
       " ('밤도깨비 김지원', 24),\n",
       " ('삼성전자 갤럭시', 23),\n",
       " ('근로자 만원', 22),\n",
       " ('구구단 세정', 21),\n",
       " ('박근혜 대통령', 21),\n",
       " ('우리 동네', 21),\n",
       " ('짠내투 김생민', 21),\n",
       " ('근로자 정규직', 20),\n",
       " ('근로자 휴가지원', 20),\n",
       " ('포토 아이들', 20),\n",
       " ('홍석천 이민우', 20),\n",
       " ('보이스피싱 인출', 19),\n",
       " ('신문 취소', 19)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()           # A list of n-Grams.\n",
    "my_dict = {}\n",
    "for a_gram in n_grams:\n",
    "    words = nltk.word_tokenize(a_gram)\n",
    "    a_nm1_gram = ' '.join(words[0:n-1])                         # (n-1)-Gram.\n",
    "    next_word = words[-1]                                       # Word after the a_nm1_gram.\n",
    "    if a_nm1_gram not in my_dict.keys():\n",
    "        my_dict[a_nm1_gram] = [next_word]                       # a_nm1_gram is a new key. So, initialize the dictionary entry.\n",
    "    else:\n",
    "        my_dict[a_nm1_gram] += [next_word]                      # an_nm1_gram is already in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음 연관 단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word (arr):\n",
    "    for i in arr:\n",
    "        print(i, \"--->\", my_dict[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여행 ---> ['가성비', '가이드', '가즈아', '가지', '강원']\n",
      "맛집 ---> ['가빈', '가심비', '각종', '갈비씨', '갈비찜']\n",
      "종합 ---> ['가성비', '강원', '검찰', '경부선', '경주']\n",
      "오늘 ---> ['개통', '걸음', '결론', '결정', '경주']\n"
     ]
    }
   ],
   "source": [
    "next_word(['여행', '맛집', '종합', '오늘'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
