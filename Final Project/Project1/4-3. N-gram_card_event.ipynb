{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from numpy.random import randint, seed\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from PIL import Image  \n",
    "from IPython.display import set_matplotlib_formats\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Python\\\\4-2) 핀테크 교육\\\\Final Project\\\\Project1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\workspace\\\\Python\\\\4-2) 핀테크 교육\\\\Final Project\\\\Project1\\\\data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\".\\data\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text=pd.read_csv(\"SH_card.csv\", encoding='CP949')\n",
    "my_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['신한 FAN 이용금액별 포인트 적립 이벤트', '신한카드 행복드림(Dream) 페스티벌!',\n",
       "       '워너원 한정판 체크카드 사용하고 경품받자!', 'L7호텔 Yellow Summer 이벤트',\n",
       "       '파크 하얏트 서울 Summer Staycation 이벤트', '「신한카드 골프 챌린지」참가 신청 안내',\n",
       "       '해외여행 최대 5% + 최대 10만원할인', 'PEUGEOT 2008 SUV Zero 이벤트',\n",
       "       '시트로엥 C4 CACTUS Zero 이벤트', '신한카드 YOOX 10~15% 추가할인'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text['text'].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_text['text'].values) - len(set(my_text['text'].values)) # 중복 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_meaning = \"이벤트|신한카드|신한\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['text'].values):\n",
    "    a_line = re.sub('[^ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)           # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)               # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 추가할인',\n",
       " '부산 송도해상케이블카 할인 ',\n",
       " ' 교통카드 캐시백 ',\n",
       " '롯데월드 키즈파크에서 마이포인트 사용하세요',\n",
       " '환경부 기념 전기차 충전 하이패스 전액지원 ',\n",
       " '올댓컬쳐 뮤지컬 최대 할인',\n",
       " ' 개편맞이 퀴즈 경품 ',\n",
       " '도시가스 자동이체신청 최대 원 캐시백 ',\n",
       " '신세계백화점에서 결제 시 신세계상품권 증정 월 ',\n",
       " '해외쇼핑 전상품 추가할인']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "# 명사 추출\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "    \n",
    "#my_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단음절 제거\n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word\n",
    "        \n",
    "#my_words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아고다 호텔', 8),\n",
       " ('창립 주년', 7),\n",
       " ('최대 캐시백', 6),\n",
       " ('하얏트 서울', 4),\n",
       " ('신세계상품권 증정', 3),\n",
       " ('아트 컬렉션', 3),\n",
       " ('연회비 캐시백', 3),\n",
       " ('올댓쇼핑 첫구', 3),\n",
       " ('주년 기념', 3),\n",
       " ('첫구 감사', 3),\n",
       " ('최대 포인트', 3),\n",
       " ('캐시백 창립', 3),\n",
       " ('캐시백 해외직구', 3),\n",
       " ('파크 하얏트', 3),\n",
       " ('포인트 적립', 3),\n",
       " ('플라자 호텔', 3),\n",
       " ('해외직구 최대', 3),\n",
       " ('감사 포인트적립', 2),\n",
       " ('개월 할부', 2),\n",
       " ('고객 감사', 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()           # A list of n-Grams.\n",
    "my_dict = {}\n",
    "for a_gram in n_grams:\n",
    "    words = nltk.word_tokenize(a_gram)\n",
    "    a_nm1_gram = ' '.join(words[0:n-1])                         # (n-1)-Gram.\n",
    "    next_word = words[-1]                                       # Word after the a_nm1_gram.\n",
    "    if a_nm1_gram not in my_dict.keys():\n",
    "        my_dict[a_nm1_gram] = [next_word]                       # a_nm1_gram is a new key. So, initialize the dictionary entry.\n",
    "    else:\n",
    "        my_dict[a_nm1_gram] += [next_word]                      # an_nm1_gram is already in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연관 단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word (arr):\n",
    "    for i in arr:\n",
    "        print(i, \"--->\", my_dict[i][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신한카드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할인 ---> ['개편맞이', '공연', '교통카드', '내차처럼', '더치페']\n",
      "캐시백 ---> ['그랜드', '롯데월드', '롯데호텔', '받으', '분기']\n",
      "호텔 ---> ['가든페스트', '라오스', '롯데호텔', '베트남', '부산']\n",
      "포인트 ---> ['국내', '네이버페', '드림', '받으', '적립']\n",
      "최대 ---> ['금액대별', '만원', '만원할인', '선착순', '설맞']\n"
     ]
    }
   ],
   "source": [
    "next_word(['할인', '캐시백', '호텔', '포인트', '최대'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 삼성카드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text=pd.read_csv(\"SS_card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_meaning = \"이벤트|삼성카드|삼성\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['text'].values):\n",
    "    a_line = re.sub('[^ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)           # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)               # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "    \n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('만원 캐시백', 11),\n",
       " ('여행 혜택', 10),\n",
       " ('혜택 여행', 9),\n",
       " ('호텔 혜택', 8),\n",
       " ('가연 미팅파티', 6),\n",
       " ('창립 주년', 6),\n",
       " ('호텔 예약', 6),\n",
       " ('화재 다이렉트', 6),\n",
       " ('겨울 여행', 5),\n",
       " ('다이렉트 오토', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()           # A list of n-Grams.\n",
    "my_dict = {}\n",
    "for a_gram in n_grams:\n",
    "    words = nltk.word_tokenize(a_gram)\n",
    "    a_nm1_gram = ' '.join(words[0:n-1])                         # (n-1)-Gram.\n",
    "    next_word = words[-1]                                       # Word after the a_nm1_gram.\n",
    "    if a_nm1_gram not in my_dict.keys():\n",
    "        my_dict[a_nm1_gram] = [next_word]                       # a_nm1_gram is a new key. So, initialize the dictionary entry.\n",
    "    else:\n",
    "        my_dict[a_nm1_gram] += [next_word]                      # an_nm1_gram is already in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word (arr):\n",
    "    for i in arr:\n",
    "        print(i, \"--->\", my_dict[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캐시백 ---> ['가연', '가족', '갤럭시', '경품', '교보라이프플래닛생명']\n",
      "여행 ---> ['가뿐', '가을여행', '계절', '고민', '고속도로']\n",
      "보험 ---> ['상식', '상품권', '연금저축보험', '캐시백']\n"
     ]
    }
   ],
   "source": [
    "next_word(['캐시백', '여행', '보험'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 국민카드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text=pd.read_csv(\"KB_card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_meaning = \"이벤트|국민카드|국민\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['text'].values):\n",
    "    a_line = re.sub('[^ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)           # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)               # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "    \n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('최대 만원', 49),\n",
       " ('만원 캐시백', 46),\n",
       " ('최대 할인', 43),\n",
       " ('국제선 항공권', 20),\n",
       " ('항공권 최대', 20),\n",
       " ('개월 무이자할부', 15),\n",
       " ('라이프샵 하나', 12),\n",
       " ('호남 충청', 12),\n",
       " ('만원 할인', 11),\n",
       " ('온라인 신규발급', 11)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()           # A list of n-Grams.\n",
    "my_dict = {}\n",
    "for a_gram in n_grams:\n",
    "    words = nltk.word_tokenize(a_gram)\n",
    "    a_nm1_gram = ' '.join(words[0:n-1])                         # (n-1)-Gram.\n",
    "    next_word = words[-1]                                       # Word after the a_nm1_gram.\n",
    "    if a_nm1_gram not in my_dict.keys():\n",
    "        my_dict[a_nm1_gram] = [next_word]                       # a_nm1_gram is a new key. So, initialize the dictionary entry.\n",
    "    else:\n",
    "        my_dict[a_nm1_gram] += [next_word]                      # an_nm1_gram is already in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word (arr):\n",
    "    for i in arr:\n",
    "        print(i, \"--->\", my_dict[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할인 ---> ['가정', '가정의달', '개월', '겨울', '고객대상']\n",
      "캐시백 ---> ['가격보고', '가온부울경카드', '가족', '간편결제', '개인사업자']\n",
      "혜택 ---> ['가을가을해', '가즈아', '개월', '게임', '경품']\n"
     ]
    }
   ],
   "source": [
    "next_word(['할인', '캐시백', '혜택'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 우리카드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text=pd.read_csv(\"WR_card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_meaning = \"이벤트|우리카드|우리\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['text'].values):\n",
    "    a_line = re.sub('[^ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)           # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)               # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "    \n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('최대 할인', 8),\n",
       " ('아고다 최대', 7),\n",
       " ('카드 정석', 5),\n",
       " ('최대 만원', 4),\n",
       " ('공사 임대주택', 3),\n",
       " ('만원 캐시백', 3),\n",
       " ('임대료 자동납부', 3),\n",
       " ('임대주택 임대료', 3),\n",
       " ('지방세 납부', 3),\n",
       " ('티켓 할인', 3)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()           # A list of n-Grams.\n",
    "my_dict = {}\n",
    "for a_gram in n_grams:\n",
    "    words = nltk.word_tokenize(a_gram)\n",
    "    a_nm1_gram = ' '.join(words[0:n-1])                         # (n-1)-Gram.\n",
    "    next_word = words[-1]                                       # Word after the a_nm1_gram.\n",
    "    if a_nm1_gram not in my_dict.keys():\n",
    "        my_dict[a_nm1_gram] = [next_word]                       # a_nm1_gram is a new key. So, initialize the dictionary entry.\n",
    "    else:\n",
    "        my_dict[a_nm1_gram] += [next_word]                      # an_nm1_gram is already in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word (arr):\n",
    "    for i in arr:\n",
    "        print(i, \"--->\", my_dict[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할인 ---> ['가정', '고객만', '공사', '구글플레', '대학']\n",
      "납부 ---> ['실속', '청구할인', '캐시백']\n",
      "캐시백 ---> ['경품', '도시가스요금', '사회보험료', '세븐일레븐', '아고다']\n"
     ]
    }
   ],
   "source": [
    "next_word(['할인', '납부', '캐시백'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하나카드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text=pd.read_csv(\"HN_card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_meaning = \"이벤트|하나카드|하나\"\n",
    "my_text_clean = []\n",
    "\n",
    "for a_line in set(my_text['text'].values):\n",
    "    a_line = re.sub('[^ㄱ-ㅣ가-힣]+', ' ', a_line) # 한글, 숫자 이외는 모두 공백으로\n",
    "    a_line = re.sub(no_meaning, '', a_line)           # 의미없는 문자 삭제\n",
    "    a_line = re.sub('\\s+', ' ', a_line)               # 잉여 스페이스를 공백 하나로\n",
    "    my_text_clean += [a_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = konlpy.tag.Hannanum()\n",
    "\n",
    "my_words = []\n",
    "for a_line, n in zip(my_text_clean, range(len(my_text_clean))):\n",
    "    #print(n, end=\" \")\n",
    "    if a_line == \" \":\n",
    "        #print(\"패스\")\n",
    "        continue\n",
    "    my_words += hannanum.nouns(a_line)\n",
    "    \n",
    "my_words_2 = [\"\"]\n",
    "for a_word in my_words:\n",
    "    if len(a_word) > 1:\n",
    "        my_words_2[0] += \" \" + a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2                                                            # Can be changed to a number equal or larger than 2.\n",
    "n_min = n\n",
    "n_max = n\n",
    "n_gram_type = 'word'                                             # n-Gram with words.\n",
    "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer = n_gram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()            # Get the n-Grams as a list.\n",
    "n_gram_cts = vectorizer.transform(my_words_2).toarray()             #  The output is an array of array.\n",
    "n_gram_cts = list(n_gram_cts[0])                                 # Convert into a simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('최대 할인', 11),\n",
       " ('쇼핑 제휴몰', 8),\n",
       " ('청구할인 혜택', 8),\n",
       " ('다이닝 서비스', 7),\n",
       " ('만원 써프라이즈', 7),\n",
       " ('문화 방법', 7),\n",
       " ('방법 월간', 7),\n",
       " ('월간 컬처', 7),\n",
       " ('체크 혜택', 6),\n",
       " ('국내 해외', 4)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for z, zz in zip(n_grams, n_gram_cts):\n",
    "    res_dict[z] = zz\n",
    "res_dict = sorted(res_dict.items(), key=(lambda x: x[1]), reverse=True)\n",
    "res_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = vectorizer.fit(my_words_2).get_feature_names()           # A list of n-Grams.\n",
    "my_dict = {}\n",
    "for a_gram in n_grams:\n",
    "    words = nltk.word_tokenize(a_gram)\n",
    "    a_nm1_gram = ' '.join(words[0:n-1])                         # (n-1)-Gram.\n",
    "    next_word = words[-1]                                       # Word after the a_nm1_gram.\n",
    "    if a_nm1_gram not in my_dict.keys():\n",
    "        my_dict[a_nm1_gram] = [next_word]                       # a_nm1_gram is a new key. So, initialize the dictionary entry.\n",
    "    else:\n",
    "        my_dict[a_nm1_gram] += [next_word]                      # an_nm1_gram is already in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word (arr):\n",
    "    for i in arr:\n",
    "        print(i, \"--->\", my_dict[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할인 ---> ['가족', '그랜드', '대표', '라로카', '롯데호텔']\n",
      "혜택 ---> ['강력한', '경품', '라발레', '레진코믹스', '롯데워터파크']\n",
      "포인트 ---> ['전환']\n"
     ]
    }
   ],
   "source": [
    "next_word(['할인', '혜택', '포인트'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
